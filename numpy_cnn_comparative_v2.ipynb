{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numpy_cnn_comparative_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODC1jzjGrGb5"
      },
      "source": [
        "Platforma pentru experimente privind modele \"low complexity\" de sisnapse (in particular sinapsa comparativa) \n",
        "GIT-ul de plecare [1] ofera cod exclusiv NUMPY (nu Tensorflow sau Keras) cu avantajul ca se poate interveni la orice nivel asupra algoritmului (de ex. inlocuirea sinapsei clasice cu sinapsa comparativa. \n",
        "\n",
        "--------------\n",
        "\n",
        "**To do:**  \n",
        "Pentru eficientizare si posibilitatea de utilizare GPU, utilizare Cupy in loc de Numpy ( alt cod cu utilizare CUPY aici  https://github.com/radu-dogaru/LB-CNN-compact-and-fast-binary-including-very-fast-ELM ) \n",
        "\n",
        "Inserarea / evaluarea unor modele noi de sinapsa (in principal cea comparativa) \n",
        "\n",
        "Nota: pentru sinapsa comparativa in aceasta etapa trebuiesc \n",
        "a) inlocuit fisierul nn.py cu cel modificat (arhiva atasata) \n",
        "b) in directorul layers inserate cfc.py si cconv.py \n",
        "(suport sinapse comparative) \n",
        "\n",
        "Radu Dogaru, 1 oct. 2021 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wayi85IT1Vs8",
        "outputId": "d9123241-a899-4ff1-c360-cc8118ae7c7b"
      },
      "source": [
        "#!git clone https://github.com/lpraat/numpyCNN\n",
        "!git clone https://github.com/radu-dogaru/numpyCNN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'numpyCNN'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 65 (delta 5), reused 0 (delta 0), pack-reused 53\u001b[K\n",
            "Unpacking objects: 100% (65/65), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwHu8nU61aJx"
      },
      "source": [
        "Recomandat ca fiind \"clar\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGKTi2Vs1kOb",
        "outputId": "545a4959-5f75-4ad2-a2db-7dd57b4c9222"
      },
      "source": [
        "cd /content/numpyCNN"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/numpyCNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTSYOwjW2rg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eI8RV8J1sPw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import mnist as my_mnist\n",
        "from src.activation import relu, softmax, identity, sigmoid\n",
        "from src.cost import softmax_cross_entropy\n",
        "from src.layers.conv import Conv\n",
        "from src.layers.cconv import CConv\n",
        "from src.layers.dropout import Dropout\n",
        "from src.layers.fc import FullyConnected\n",
        "from src.layers.cfc import CFullyConnected\n",
        "from src.layers.flatten import Flatten\n",
        "from src.layers.pool import Pool\n",
        "from src.nn import NeuralNetwork\n",
        "from src.optimizer import adam, rmsprop\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0DmlWqvQ_Wo",
        "outputId": "3bd7123a-c761-4d5c-bb48-4ff95b5e0f4c"
      },
      "source": [
        "# This cell expands the usual datasest available in Keras \n",
        "!pip install extra-keras-datasets\n",
        "from extra_keras_datasets import emnist, svhn, stl10, usps"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: extra-keras-datasets in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->extra-keras-datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->extra-keras-datasets) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq9VTMDKAgDo"
      },
      "source": [
        "Datasets loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZd_z1P2AWvy",
        "outputId": "41b9e1b2-59b4-43bc-d62e-01c98ea2b49c"
      },
      "source": [
        "dataset='usps' # mnist or f-mnist or cifar10 \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "dformat='channels_last'\n",
        "\n",
        "from keras.datasets import mnist, cifar10, cifar100, fashion_mnist\n",
        "\n",
        "if dataset=='mnist':\n",
        "      (x_train, y_train), (x_test, y_test) = mnist.load_data() # incarca date nescalate \n",
        "elif  dataset=='cifar10': \n",
        "      (x_train, y_train), (x_test, y_test) = cifar10.load_data() # incarca date nescalate \n",
        "      y_test=y_test.T\n",
        "elif  dataset=='cifar100': \n",
        "      (x_train, y_train), (x_test, y_test) = cifar100.load_data() # incarca date nescalate \n",
        "elif dataset=='f-mnist':\n",
        "      (x_train, y_train), (x_test, y_test) =  fashion_mnist.load_data()\n",
        "elif dataset=='usps':\n",
        "    (x_train, y_train), (x_test, y_test) =  usps.load_data()\n",
        "    y_train=y_train.astype('int32')\n",
        "    x_train=256*x_train; x_test=256*x_test\n",
        "\n",
        "if (np.ndim(x_train)==3):   # E.g.  MNIST or F-MNIST  \n",
        "      x_train=np.reshape(x_train, [np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2], 1]) \n",
        "      x_test=np.reshape(x_test, [np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2], 1] ) \n",
        "    # place a  1 in the end to keep it compatible with kernel in conv2d \n",
        "    # scaling in ([0,1])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /=255 \n",
        "num_classes=np.max(y_train)+1\n",
        "input_shape=np.shape(x_train)[1:4]\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# y_train in format categoric si y_test in format normal "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Loading dataset = usps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\n",
            "6586368/6579383 [==============================] - 1s 0us/step\n",
            "6594560/6579383 [==============================] - 1s 0us/step\n",
            "Downloading data from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\n",
            "1835008/1831726 [==============================] - 1s 0us/step\n",
            "1843200/1831726 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please cite the following paper when using or referencing this Extra Keras Dataset:\n",
            "WARNING:root:Hull, J. J. (1994). A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5), 550-554.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okebQbw_BM_J"
      },
      "source": [
        "Defining the CNN model (based on existent layers) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuqVSWkCSCjt"
      },
      "source": [
        "cnn = NeuralNetwork(\n",
        "        input_dim=input_shape,\n",
        "        layers=[\n",
        "            CConv(3, 1, 10, activation=relu),\n",
        "            Pool(2, 2, 'max'),\n",
        "            #CConv(3, 1, 8, activation=relu),\n",
        "\n",
        "            #Conv(3, 1, 4, activation=relu),\n",
        "            #Conv(3, 1, 12, activation=relu),\n",
        "            #Conv(3, 1, 32, activation=relu),\n",
        "            #Conv(5, 1, 32, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 8, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 8, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 64, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            \n",
        "            #Dropout(0.75),\n",
        "            Flatten(),\n",
        "            #FullyConnected(500, relu),\n",
        "            #Dropout(0.9),\n",
        "            #CFullyConnected(10, softmax),\n",
        "            FullyConnected(10, softmax),\n",
        "        ],\n",
        "        cost_function=softmax_cross_entropy,\n",
        "        optimizer=rmsprop\n",
        "    )\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyFcFnzzB_mP"
      },
      "source": [
        "Training the model defined above "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Y4RW2ZB975",
        "outputId": "570b97c4-82e5-485f-b6e2-78a76b8ffb6c"
      },
      "source": [
        "cnn.train(x_train, y_train,\n",
        "              mini_batch_size=100,\n",
        "              learning_rate=0.01,\n",
        "              num_epochs=20,\n",
        "              validation_data=(x_test, y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training [batch_size=100, learning_rate=0.01]\n",
            "Epoch 1\n",
            "Progress 100.0%\n",
            "Cost after epoch 1: 1.1397604990567676\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9018435475834579\n",
            "Latentza pe set test:  0.42645433499080126 mili-secunde\n",
            "Durata epoca:  7.483052730560303  secunde\n",
            "Epoch 2\n",
            "Progress 100.0%\n",
            "Cost after epoch 2: 0.17880336769001648\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9277528649725959\n",
            "Latentza pe set test:  0.416891932428091 mili-secunde\n",
            "Durata epoca:  7.313705205917358  secunde\n",
            "Epoch 3\n",
            "Progress 100.0%\n",
            "Cost after epoch 3: 0.10103575526254227\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8963627304434479\n",
            "Latentza pe set test:  0.4173325375650555 mili-secunde\n",
            "Durata epoca:  7.381204128265381  secunde\n",
            "Epoch 4\n",
            "Progress 100.0%\n",
            "Cost after epoch 4: 0.05762590552930371\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9392127553562531\n",
            "Latentza pe set test:  0.42174642930650924 mili-secunde\n",
            "Durata epoca:  7.39761209487915  secunde\n",
            "Epoch 5\n",
            "Progress 100.0%\n",
            "Cost after epoch 5: 0.051264737733557525\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9451918285999004\n",
            "Latentza pe set test:  0.4089396571305241 mili-secunde\n",
            "Durata epoca:  7.300924777984619  secunde\n",
            "Epoch 6\n",
            "Progress 100.0%\n",
            "Cost after epoch 6: 0.032527941405086525\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9362232187344295\n",
            "Latentza pe set test:  0.4266556899940783 mili-secunde\n",
            "Durata epoca:  7.332276344299316  secunde\n",
            "Epoch 7\n",
            "Progress 100.0%\n",
            "Cost after epoch 7: 0.024801913960946064\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9446935724962631\n",
            "Latentza pe set test:  0.40025133723100736 mili-secunde\n",
            "Durata epoca:  7.301952600479126  secunde\n",
            "Epoch 8\n",
            "Progress 100.0%\n",
            "Cost after epoch 8: 0.024112849783190005\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9412057797708022\n",
            "Latentza pe set test:  0.4207040162364464 mili-secunde\n",
            "Durata epoca:  7.416326284408569  secunde\n",
            "Epoch 9\n",
            "Progress 100.0%\n",
            "Cost after epoch 9: 0.015645505454450218\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9446935724962631\n",
            "Latentza pe set test:  0.41580295230122305 mili-secunde\n",
            "Durata epoca:  7.381463050842285  secunde\n",
            "Epoch 10\n",
            "Progress 100.0%\n",
            "Cost after epoch 10: 0.014913347702938359\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9466865969108121\n",
            "Latentza pe set test:  0.42366981506347656 mili-secunde\n",
            "Durata epoca:  7.322801828384399  secunde\n",
            "Epoch 11\n",
            "Progress 100.0%\n",
            "Cost after epoch 11: 0.012396156846727322\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9481813652217239\n",
            "Latentza pe set test:  0.3973972043054996 mili-secunde\n",
            "Durata epoca:  7.25960636138916  secunde\n",
            "Epoch 12\n",
            "Progress 100.0%\n",
            "Cost after epoch 12: 0.012340900774108416\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.942700548081714\n",
            "Latentza pe set test:  0.39869704291662533 mili-secunde\n",
            "Durata epoca:  7.216660976409912  secunde\n",
            "Epoch 13\n",
            "Progress 100.0%\n",
            "Cost after epoch 13: 0.009526133132047913\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9451918285999004\n",
            "Latentza pe set test:  0.3990301399249964 mili-secunde\n",
            "Durata epoca:  7.1954827308654785  secunde\n",
            "Epoch 14\n",
            "Progress 100.0%\n",
            "Cost after epoch 14: 0.007452061265466408\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9446935724962631\n",
            "Latentza pe set test:  0.3911649402719145 mili-secunde\n",
            "Durata epoca:  7.195072650909424  secunde\n",
            "Epoch 15\n",
            "Progress 100.0%\n",
            "Cost after epoch 15: 0.009426169521247736\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9417040358744395\n",
            "Latentza pe set test:  0.3884183154750001 mili-secunde\n",
            "Durata epoca:  7.1765711307525635  secunde\n",
            "Epoch 16\n",
            "Progress 100.0%\n",
            "Cost after epoch 16: 0.006340119538341839\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9436970602889886\n",
            "Latentza pe set test:  0.3900896213989562 mili-secunde\n",
            "Durata epoca:  7.1463587284088135  secunde\n",
            "Epoch 17\n",
            "Progress 100.0%\n",
            "Cost after epoch 17: 0.005082359777627373\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9481813652217239\n",
            "Latentza pe set test:  0.3941988077814685 mili-secunde\n",
            "Durata epoca:  7.241095781326294  secunde\n",
            "Epoch 18\n",
            "Progress 100.0%\n",
            "Cost after epoch 18: 0.004475283074193206\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9511709018435476\n",
            "Latentza pe set test:  0.4111992285271812 mili-secunde\n",
            "Durata epoca:  7.341503858566284  secunde\n",
            "Epoch 19\n",
            "Progress 100.0%\n",
            "Cost after epoch 19: 0.00627738139276915\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.942700548081714\n",
            "Latentza pe set test:  0.39743830686074083 mili-secunde\n",
            "Durata epoca:  7.201009273529053  secunde\n",
            "Epoch 20\n",
            "Progress 100.0%\n",
            "Cost after epoch 20: 0.0050165142063403065\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9491778774289985\n",
            "Latentza pe set test:  0.39882189489800834 mili-secunde\n",
            "Durata epoca:  7.211522340774536  secunde\n",
            "Finished training\n",
            "Cea mai buna acuratete pe set validare:  0.9511709018435476 In epoca:  17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vQ7XM_21ZBM"
      },
      "source": [
        "Obs: \n",
        "\n",
        "With CPU (GPU is not supported in NUMPY) the training lasts long \n",
        "\n",
        "Code can be used as a starting template for further improvements. It may be also conveninet for TinyML implements (low complexity microcontrollers supporting Python / Numpy) \n",
        "\n",
        "R. Dogaru, Oct. 2021"
      ]
    }
  ]
}