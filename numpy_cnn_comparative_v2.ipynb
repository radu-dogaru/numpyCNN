{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numpy_cnn_comparative_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODC1jzjGrGb5"
      },
      "source": [
        "Platforma pentru experimente privind modele \"low complexity\" de sisnapse (in particular sinapsa comparativa) \n",
        "GIT-ul de plecare [1] ofera cod exclusiv NUMPY (nu Tensorflow sau Keras) cu avantajul ca se poate interveni la orice nivel asupra algoritmului (de ex. inlocuirea sinapsei clasice cu sinapsa comparativa. \n",
        "\n",
        "--------------\n",
        "\n",
        "**To do:**  \n",
        "Pentru eficientizare si posibilitatea de utilizare GPU, utilizare Cupy in loc de Numpy ( alt cod cu utilizare CUPY aici  https://github.com/radu-dogaru/LB-CNN-compact-and-fast-binary-including-very-fast-ELM ) \n",
        "\n",
        "Inserarea / evaluarea unor modele noi de sinapsa (in principal cea comparativa) \n",
        "\n",
        "Nota: pentru sinapsa comparativa in aceasta etapa trebuiesc \n",
        "a) inlocuit fisierul nn.py cu cel modificat (arhiva atasata) \n",
        "b) in directorul layers inserate cfc.py si cconv.py \n",
        "(suport sinapse comparative) \n",
        "\n",
        "Radu Dogaru, 1 oct. 2021 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wayi85IT1Vs8",
        "outputId": "885f8653-c025-4769-cf0b-83652106c5f4"
      },
      "source": [
        "#!git clone https://github.com/lpraat/numpyCNN\n",
        "!git clone https://github.com/radu-dogaru/numpyCNN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'numpyCNN'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 49 (delta 5), reused 0 (delta 0), pack-reused 35\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwHu8nU61aJx"
      },
      "source": [
        "Recomandat ca fiind \"clar\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGKTi2Vs1kOb",
        "outputId": "6f8b2d1a-1bd0-4371-94e1-a789ed09262d"
      },
      "source": [
        "cd /content/numpyCNN"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/numpyCNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTSYOwjW2rg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eI8RV8J1sPw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import mnist as my_mnist\n",
        "from src.activation import relu, softmax\n",
        "from src.cost import softmax_cross_entropy\n",
        "from src.layers.conv import Conv\n",
        "from src.layers.cconv import CConv\n",
        "from src.layers.dropout import Dropout\n",
        "from src.layers.fc import FullyConnected\n",
        "from src.layers.cfc import CFullyConnected\n",
        "from src.layers.flatten import Flatten\n",
        "from src.layers.pool import Pool\n",
        "from src.nn import NeuralNetwork\n",
        "from src.optimizer import adam, rmsprop\n",
        "\n",
        "\n",
        "#-------------- cele de mai jos nu mai sunt utilizate --------------------\n",
        "def one_hot(x, num_classes=10):\n",
        "    out = np.zeros((x.shape[0], num_classes))\n",
        "    out[np.arange(x.shape[0]), x[:, 0]] = 1\n",
        "    return out\n",
        "\n",
        "\n",
        "def preprocess(x_train, y_train, x_test, y_test):\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype(np.float32)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype(np.float32)\n",
        "    y_train = one_hot(y_train.reshape(y_train.shape[0], 1))\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0DmlWqvQ_Wo",
        "outputId": "a490f2e8-f331-4a1e-93b3-180983eed4d1"
      },
      "source": [
        "# This cell expands the usual datasest available in Keras \n",
        "!pip install extra-keras-datasets\n",
        "from extra_keras_datasets import emnist, svhn, stl10, usps"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting extra-keras-datasets\n",
            "  Downloading extra_keras_datasets-1.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->extra-keras-datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->extra-keras-datasets) (1.0.1)\n",
            "Installing collected packages: extra-keras-datasets\n",
            "Successfully installed extra-keras-datasets-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuqVSWkCSCjt",
        "outputId": "15f1b77c-77c1-4d1d-b277-1aa8358baea3"
      },
      "source": [
        "\n",
        "'''\n",
        "my_mnist.init()  #Se inlocuieste pentru ca este f. lent \n",
        "x_train, y_train, x_test, y_test = preprocess(*my_mnist.load())\n",
        "input_shape=np.shape(x_train)[1:4]\n",
        "'''\n",
        "dataset='usps' # mnist or f-mnist or cifar10 \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "dformat='channels_last'\n",
        "\n",
        "from keras.datasets import mnist, cifar10, cifar100, fashion_mnist\n",
        "\n",
        "if dataset=='mnist':\n",
        "      (x_train, y_train), (x_test, y_test) = mnist.load_data() # incarca date nescalate \n",
        "elif  dataset=='cifar10': \n",
        "      (x_train, y_train), (x_test, y_test) = cifar10.load_data() # incarca date nescalate \n",
        "      y_test=y_test.T\n",
        "elif  dataset=='cifar100': \n",
        "      (x_train, y_train), (x_test, y_test) = cifar100.load_data() # incarca date nescalate \n",
        "elif dataset=='f-mnist':\n",
        "      (x_train, y_train), (x_test, y_test) =  fashion_mnist.load_data()\n",
        "elif dataset=='usps':\n",
        "    (x_train, y_train), (x_test, y_test) =  usps.load_data()\n",
        "    y_train=y_train.astype('int32')\n",
        "    x_train=256*x_train; x_test=256*x_test\n",
        "\n",
        "if (np.ndim(x_train)==3):   # E.g.  MNIST or F-MNIST  \n",
        "      x_train=np.reshape(x_train, [np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2], 1]) \n",
        "      x_test=np.reshape(x_test, [np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2], 1] ) \n",
        "    # place a  1 in the end to keep it compatible with kernel in conv2d \n",
        "    # scaling in ([0,1])\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /=255 \n",
        "num_classes=np.max(y_train)+1\n",
        "input_shape=np.shape(x_train)[1:4]\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "#y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "# y_train in format categoric si y_test in format normal \n",
        "\n",
        "\n",
        "cnn = NeuralNetwork(\n",
        "        input_dim=input_shape,\n",
        "        layers=[\n",
        "            CConv(3, 1, 8, activation=relu),\n",
        "            Pool(2, 2, 'max'),\n",
        "            CConv(3, 1, 8, activation=relu),\n",
        "\n",
        "            #Conv(3, 1, 4, activation=relu),\n",
        "            #Conv(3, 1, 12, activation=relu),\n",
        "            #Conv(3, 1, 32, activation=relu),\n",
        "            #Conv(5, 1, 32, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 8, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 8, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Conv(3, 1, 64, activation=relu),\n",
        "            #Pool(2, 2, 'max'),\n",
        "            #Dropout(0.75),\n",
        "            Flatten(),\n",
        "            #FullyConnected(128, relu),\n",
        "            #Dropout(0.9),\n",
        "            #CFullyConnected(10, softmax),\n",
        "            FullyConnected(10, softmax),\n",
        "        ],\n",
        "        cost_function=softmax_cross_entropy,\n",
        "        optimizer=rmsprop\n",
        "    )\n",
        "\n",
        "cnn.train(x_train, y_train,\n",
        "              mini_batch_size=50,\n",
        "              learning_rate=0.01,\n",
        "              num_epochs=20,\n",
        "              validation_data=(x_test, y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Loading dataset = usps\n",
            "WARNING:root:Please cite the following paper when using or referencing this Extra Keras Dataset:\n",
            "WARNING:root:Hull, J. J. (1994). A database for handwritten text recognition research. IEEE Transactions on pattern analysis and machine intelligence, 16(5), 550-554.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training [batch_size=50, learning_rate=0.01]\n",
            "Epoch 1\n",
            "Progress 100.0%\n",
            "Cost after epoch 1: 2.5503752633879575\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8799202790234181\n",
            "Latentza pe set test:  0.5668281142724707 mili-secunde\n",
            "Durata epoca:  7.72726845741272  secunde\n",
            "Epoch 2\n",
            "Progress 100.0%\n",
            "Cost after epoch 2: 0.7018742794230479\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8998505231689088\n",
            "Latentza pe set test:  0.5668420131134049 mili-secunde\n",
            "Durata epoca:  7.722208261489868  secunde\n",
            "Epoch 3\n",
            "Progress 100.0%\n",
            "Cost after epoch 3: 0.539672265280495\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8993522670652716\n",
            "Latentza pe set test:  0.5714501322474003 mili-secunde\n",
            "Durata epoca:  7.738502264022827  secunde\n",
            "Epoch 4\n",
            "Progress 100.0%\n",
            "Cost after epoch 4: 0.46733982191772494\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8829098156452416\n",
            "Latentza pe set test:  0.56641506923035 mili-secunde\n",
            "Durata epoca:  7.7512617111206055  secunde\n",
            "Epoch 5\n",
            "Progress 100.0%\n",
            "Cost after epoch 5: 0.4200548235267044\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.909317389138017\n",
            "Latentza pe set test:  0.5616269779252839 mili-secunde\n",
            "Durata epoca:  7.63861870765686  secunde\n",
            "Epoch 6\n",
            "Progress 100.0%\n",
            "Cost after epoch 6: 0.3765631615658204\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.8968609865470852\n",
            "Latentza pe set test:  0.5783382552621611 mili-secunde\n",
            "Durata epoca:  7.769378662109375  secunde\n",
            "Epoch 7\n",
            "Progress 100.0%\n",
            "Cost after epoch 7: 0.33984070658335835\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9207772795216741\n",
            "Latentza pe set test:  0.5713332394313385 mili-secunde\n",
            "Durata epoca:  7.790109157562256  secunde\n",
            "Epoch 8\n",
            "Progress 100.0%\n",
            "Cost after epoch 8: 0.31699924236166904\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.916791230692576\n",
            "Latentza pe set test:  0.5760448277145105 mili-secunde\n",
            "Durata epoca:  7.74261474609375  secunde\n",
            "Epoch 9\n",
            "Progress 100.0%\n",
            "Cost after epoch 9: 0.2567022460942094\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.922272047832586\n",
            "Latentza pe set test:  0.5756377223480027 mili-secunde\n",
            "Durata epoca:  7.744454383850098  secunde\n",
            "Epoch 10\n",
            "Progress 100.0%\n",
            "Cost after epoch 10: 0.23855679643822092\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9207772795216741\n",
            "Latentza pe set test:  0.5791802636770449 mili-secunde\n",
            "Durata epoca:  7.753406047821045  secunde\n",
            "Epoch 11\n",
            "Progress 100.0%\n",
            "Cost after epoch 11: 0.1944682365052344\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.916791230692576\n",
            "Latentza pe set test:  0.6074957664653206 mili-secunde\n",
            "Durata epoca:  7.8348071575164795  secunde\n",
            "Epoch 12\n",
            "Progress 100.0%\n",
            "Cost after epoch 12: 0.19064421434397344\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9192825112107623\n",
            "Latentza pe set test:  0.5800318943664217 mili-secunde\n",
            "Durata epoca:  7.75900411605835  secunde\n",
            "Epoch 13\n",
            "Progress 100.0%\n",
            "Cost after epoch 13: 0.19072658284574903\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9247633283507724\n",
            "Latentza pe set test:  0.588308238365428 mili-secunde\n",
            "Durata epoca:  7.771340847015381  secunde\n",
            "Epoch 14\n",
            "Progress 100.0%\n",
            "Cost after epoch 14: 0.1662076170830595\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9322371699053313\n",
            "Latentza pe set test:  0.5810783464159013 mili-secunde\n",
            "Durata epoca:  7.738914728164673  secunde\n",
            "Epoch 15\n",
            "Progress 100.0%\n",
            "Cost after epoch 15: 0.12592214777184366\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9312406576980568\n",
            "Latentza pe set test:  0.5738981101544983 mili-secunde\n",
            "Durata epoca:  7.808738708496094  secunde\n",
            "Epoch 16\n",
            "Progress 100.0%\n",
            "Cost after epoch 16: 0.1930141287283415\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9237668161434978\n",
            "Latentza pe set test:  0.5646481345290263 mili-secunde\n",
            "Durata epoca:  7.688597917556763  secunde\n",
            "Epoch 17\n",
            "Progress 100.0%\n",
            "Cost after epoch 17: 0.1340272598327517\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9312406576980568\n",
            "Latentza pe set test:  0.5788457411465268 mili-secunde\n",
            "Durata epoca:  7.78352427482605  secunde\n",
            "Epoch 18\n",
            "Progress 100.0%\n",
            "Cost after epoch 18: 0.14452652010512232\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9357249626307922\n",
            "Latentza pe set test:  0.5671880586146121 mili-secunde\n",
            "Durata epoca:  7.699582576751709  secunde\n",
            "Epoch 19\n",
            "Progress 100.0%\n",
            "Cost after epoch 19: 0.11865461578148172\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9287493771798705\n",
            "Latentza pe set test:  0.5657535081605859 mili-secunde\n",
            "Durata epoca:  7.702542781829834  secunde\n",
            "Epoch 20\n",
            "Progress 100.0%\n",
            "Cost after epoch 20: 0.08505585091826157\n",
            "Computing accuracy on validation set...\n",
            "Accuracy on validation set:  0.9357249626307922\n",
            "Latentza pe set test:  0.5816230146696036 mili-secunde\n",
            "Durata epoca:  7.736217975616455  secunde\n",
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vQ7XM_21ZBM"
      },
      "source": [
        "Obs: \n",
        "\n",
        "Fiind NUMPY antrenarea per epoca dureaza f. mult \n",
        "\n",
        "Totusi codul este destul de \"curat\" si poate fi baza de plecare in modificarea formei de convolutie \n",
        "\n",
        "Se poate incerca trecerea la CUPY si utilizarea sinapsei comparative (caz in care trebuie umblat si la partea de gradient) "
      ]
    }
  ]
}